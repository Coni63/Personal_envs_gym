import math
import gym
import random
from gym import spaces, logger
from gym.utils import seeding
import numpy as np

class SlidingEnv(gym.Env):
    """
        Description:
            A sliding puzzle where the objective if to "sort" the grid in ascending order. The empty tile is the one
            with a value of 0.

        Observation:
            Type: Box(16)
            Num	    Observation                 Min         Max
            0-16    Value of the cell           0           1

        Actions:
            Type: Discrete(4)
            Num	Action
            0	move up
            1	move right
            2   move down
            3   move left

        Note:
            The grid is generated by row. if the grid is 3x3, the 4th element of the observation (index = 3) will to one
            of the row 2 and column 1.

        Reward:
            To avoid sparse reward problem with only a reward when solved. I divided the reward in multiple smaller one:
                - +1 if the move reduce the sum of the manhattan distance from every block to their final position
                - -1 if the move is invalid (creates no moves, for example moving up if you are on first row)
                - 0 if the move is valid but increase the the sum of the manhattan distance from every block to
                their final position
                - -10 if the move is the invert to the previous one (create oscillating algorithm to minimize the negative reward)
                - +1000 if reaches the end

        Starting State:
            Every grid are generated randomly. But every generated grid can be solved (follows Sam Loyd's rule).
            The number of move is limited to 200 so you may not reach the complete end based on the size of the grid
            and the shuffle. Nevertheless, for a 4x4 grid, in average 80 moves are required to solve it.

        Episode Termination:
            - 200 rounds done
            - grid is solved
        """

    metadata = {
        'render.modes': ['human', 'rgb_array'],
        'video.frames_per_second': 50
    }

    def __init__(self, n=4):
        super().__init__()
        self.size = n
        self.metric = 0
        self.iterations = 0
        self.max_iter = 200
        self.seed()
        self.trans_tiles = {}
        self.old_action = None
        self.viewer = None
        self.obs = None
        self.reset()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def __repr__(self):
        return str(self.obs.reshape(self.size, self.size))

    def get_observation(self):
        return self.obs / (self.size ** 2 - 1)

    def setGrid(self, grid):
        """
        :param grid: list of self.size ** 2 with expected values
        :return: if grid is valid, set the observation to this grid. Do not use to solve puzzle :D
        """
        if len(grid) != self.size**2:
            raise Exception('Wrong input shape provided')

        if sum(grid) != (self.size**2-1)* self.size**2 / 2:
            raise Exception('Wrong input provided (incorrect checksum)')

        if sorted(grid) != list(range(self.size**2)):
            raise Exception('Wrong input provided (must be only integer from 0 to size*size)')

        if self.compute_metric_Loyd(np.array(grid)) == 0:
            raise Exception('Grid is already sorted')

        if self.compute_metric_Loyd(np.array(grid)) % 2 == 1:
            raise Exception("Grid can't be solved (does not follow Sam Loyd's rule")

        self.obs = np.array(grid)
        self.compute_metric()
        self.iterations = 0
        self.old_action = None

    def generate(self, n):
        grid = np.arange(self.size * self.size)
        while True:
            np.random.shuffle(grid)
            loyd = self.compute_metric_Loyd(grid)
            if loyd % 2 == 0 and loyd > 0:
                break
        return grid

    def reset(self):
        self.obs = self.generate(self.size)
        self.compute_metric()
        self.iterations = 0
        self.old_action = None
        return self.get_observation()

    def compute_metric_Loyd(self, grid):
        # Verify if grid follow Sam Loyd's rule to be sure it can be solved
        distance = 0
        for i in range(1, self.size * self.size):
            for j in range(i + 1, self.size * self.size):
                if np.where(grid == i) > np.where(grid == j):
                    distance += 1
        return distance

    def compute_metric(self):
        # compute manhattan distance for every pieces to final position
        distance = 0
        for i in range(1, self.size * self.size):
            idx_th = ((i - 1) // self.size, (i - 1) % self.size)
            pos_i = int(np.where(self.obs == i)[0])
            idx_real = (pos_i // self.size, pos_i % self.size)
            distance += abs(idx_th[0] - idx_real[0]) + abs(idx_th[1] - idx_real[1])
        self.metric = distance
        return distance

    def swap(self, i, j):
        self.obs[i], self.obs[j] = self.obs[j], self.obs[i]

    def step(self, action):
        index_0 = np.argmin(self.obs)
        old_metric = self.metric
        new_metric = self.metric

        if action == 0:  # up
            if index_0 >= self.size:
                self.swap(index_0, index_0 - self.size)
                new_metric = self.compute_metric()
        elif action == 1:  # right
            if index_0 % self.size < self.size - 1:
                self.swap(index_0, index_0 + 1)
                new_metric = self.compute_metric()
        elif action == 2:  # down
            if index_0 <= self.size * (self.size - 1) - 1:
                self.swap(index_0, index_0 + self.size)
                new_metric = self.compute_metric()
        else:  # left
            if index_0 % self.size > 0:
                self.swap(index_0, index_0 - 1)
                new_metric = self.compute_metric()

        if new_metric == 0:
            reward = 1000
        elif new_metric < old_metric:
            reward = 1
        elif new_metric == old_metric:  # no moves
            reward = -1
        else:
            reward = 0

        if self.old_action is not None:
            if abs(action - self.old_action) == 2:
                reward -= 10
        self.old_action = action

        self.iterations += 1

        info = None

        if self.metric == 0 or self.iterations == self.max_iter:
            done = True
        else:
            done = False

        return self.get_observation(), reward, done, info

    def render(self, mode='human'):
        screen_width = 100*self.size
        screen_height = 100*self.size

        if self.viewer is None:
            from gym.envs.classic_control import rendering
            self.viewer = rendering.Viewer(screen_width, screen_height)
            half_size = 45

            for i, elem in enumerate(self.obs):
                square = rendering.FilledPolygon([(-half_size, -half_size),
                                                  (-half_size, half_size),
                                                  (half_size, half_size),
                                                  (half_size, -half_size)])
                if elem == 0:
                    square.set_color(1., 1., 1.)
                else:
                    decrease_factor = (elem + 4)/(self.size**2 + 4)
                    square.set_color(0. * decrease_factor, 1. * decrease_factor, 1. * decrease_factor)
                tile = rendering.Transform()
                square.add_attr(tile)

                self.trans_tiles[elem] = tile

                self.viewer.add_geom(square)

        for i, elem in enumerate(self.obs):
            t_x = 100*(i % self.size)+50
            t_y = 100*(i // self.size)+50
            self.trans_tiles[elem].set_translation(t_x, t_y)

        return self.viewer.render(return_rgb_array=mode == 'rgb_array')

    def close(self):
        if self.viewer: self.viewer.close()


if __name__ == "__main__":
    env = Taquin(10)
    env.render()
    while True:
        obs, reward, done, info = env.step(random.randrange(4))
        env.render()